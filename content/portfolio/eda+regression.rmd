---
title: "EDA and Regression Example"
author: "Team: SnickR"
date: "April 2018"
output: html_document
---
```{r options, include=FALSE}
#rmd output document options: html_document, word_document, pdf_document
#see chunk options at: https://yihui.name/knitr/options/
knitr::opts_chunk$set(
  echo=TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 8,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE)
```

# ETC1010 Assignment 3, Melbourne housing data

## Exercise

- Read in the Melbourne housing data (originally pulled from https://www.kaggle.com/anthonypino/melbourne-housing-market)
- Focus ONLY on the variables Price, Rooms, Type, Distance, Bedroom2, Bathroom 
- Make a missing values summary, and a strategy for dealing with the missings
- Make some good plots summarising the relationship between price
- Build your model, explaining your choices of transformations, variable selection, interactions 
- Summarise your final model, and justify why you think it is the best

## The data

This data set contains 34,857 observations of housing information in Melbourne proper from January of 2016. We were asked to look specifically at 6 variables; Price, Rooms, Type, Distance, Bedroom2, Bathroom; let's start with a short description of the variables:

- Price: price in dollars (assumed to be in AUD valued circa Jan 2016)
- Rooms: Number of rooms
- Type: h - house, t - townhouse, u - unit
- Distance: distance from CBD (assumed to be km)
- Bedroom2: Scraped # of Bedrooms (from different source)
- Bathroom: Number of Bathrooms


## Outline

1) Load and subset data
2) View missingness
3) Explore variables
4) Linear models
5) Residuals
7) Log model
8) square root model
9) Interpretation of log model

### 1) Load and subset data 
```{r Load and subset data, results='hide'}
library(tidyverse)
h <- read_csv("data/Melbourne_housing_FULL.csv")
h <- as_tibble(select(c(Price, Rooms, Distance, Bedroom2, Bathroom, Type)))
h
```

### 2) View missingness
```{r View missingness}
library(visdat)
visdat::vis_dat(h, palette = "cb_safe")
visdat::vis_miss(h, sort_miss=TRUE) + theme(aspect.ratio=1)
h <- h[complete.cases(h), ] #20800 x 6. lost 14,057 rows, 40.3%
```
A fair amount missing. Will use `complete.cases` for now to get moving, ideally we would do imputation (perhaps knn) to estimate the missing values. Using `complete.cases` removes 14,057 observations, some 40.3% of our data. Note that we would still have to throw out 21.8% of our data even with imputation as explanatory variables (y's) shouldn't be imputed. We still have a rather large sample size of `n = 20,800` to model off of.

#### 3) Explore variables
```{r Explore variables}
library(GGally)
GGally::ggpairs(h[, 1:6])
```

 `GGally::ggpairs()` summarizes numeric variables and categorical variables with a low number of unique levels well. 

Several observations about the pairs:
- data density prohibits seeing an clearly non-linear trends
-- later we will use ggplot2's `alpha = .05` and/or `geom_jitter` to see through some of this density 
- extreme values especially in Price, Rooms, Bedroom2, and Bathroom
-- we will want to log these variables at some point
- we are looking at mostly house listings (`Type = h`).
- correlation between Bedroom2 and Rooms is very high, `Corr = .959`. This makes sense, as the number of bedrooms should always be reflected by the number of rooms.
-- because the correlation of these variables is so high, it's likely that one of these variables will not be signigicant in our regression models. ie) the unique information held within number of bedrooms will be small, given that most of the information is already held within the number of rooms.

### 4) Linear models
```{r Linear models}
library(broom)
m1 <- lm(Price ~ ., data = h)
summary(m1) # note that Bedroom2 is the only non-significant term.
glance(m1)

m2 <- lm(Price ~ .-Bedroom2, data = h)
summary(m2)
glance(m2)
```
Looking at the `summary()` of the full model, Bedroom2 is the only non-significant variable. This seems quite shocking at first, but viewed in the context that it has corr = .959 with Rooms this seem more intuitive. The other variables are hands down relevant to the model as seen by their near 0 p values; all of which are below 2*10^(-16)! We remove Bedroom2 from the model and check again. The `summary()` of our 2nd model is very similar.

Viewing `glance()` is less impressive. Adj r^2 = .418, AIC > 600,000, deviance > 5.15e+15. While discouraging we need to take these in context the range of the Prices is huge, we will have large deviance terms, even adj r^2 doesn't seem bad, when considering 5 simple variables can describe over 40% of the variation in Price. The second model costs us negligible losses to drop 1 of our 6 variables.

### 5) Residuals
```{r Residuals}
hm2 <- augment(m2, h) #housing model
ggplot(hm2, aes(x=.fitted, y=.resid)) + geom_point(alpha = .05)
```
Several comments on the residual plot of the 2nd model.

- fitted values well below zero. Intuitively illogical, not sure how to constrain the model to have a lower bound near min(Price).
- increasing variance, technically against the assumptions of the linear models, at least it fans out relatively evenly.
- the mass of the observations seems to have a negative slope. Extreme values may well be having a huge effect on the sum of squares. 

Below, we'll look into removing some of the exteme value to see if this changes the slope/placement of the mass. Specifically, we fail to implement winsorizing the data and then turn to removing a small number of points with relatively high `.cooksd`, Cook's Distance, a measure of observation "extreme valueness".

### 6) Remove extreme values (not advisable)
```{r Residuals and re-modeling 2}
#library(robustHD)
#w <- h
#w[, -6] <- sapply(w[,-6], robustHD::winsorize)
#m3 <- lm(Price ~ .-Bedroom2, data = w)
###won't lm on winsorized data. let's manually chop off the top end of cooksd.

ggplot(hm2, aes(x=.cooksd), xlim=c(0,.005)) + geom_density()
quantile(hm2$.cooksd, probs = seq(.9,1,.01)) #huge jump for for last 1%!
quantile(hm2$.cooksd, probs = seq(.99,1,.001)) #huge jump for for last 0.1%!!!
plot(quantile(hm2$.cooksd, probs = seq(.998,1,.0001))) #even 0.01% is quick!
#lets remove cooksd >.001

h2 <- filter(hm2, .cooksd <.001)[,1:6] #removed 162 obs, 0.78%
m3 <- lm(Price ~ .-Bedroom2, data = h2)
glance(m2) #model to beat
glance(m3) #gives us a 23.9% reduction in model variance!
h2m3 <- augment(m3, h2)
#ggplot(hm2, aes(x=.fitted, y=.resid)) + geom_point(alpha = .05) #model to beat.
ggplot(h2m3, aes(x=.fitted, y=.resid)) + geom_point(alpha = .05)
```

First we look for a sweet spot of observations to remove. Eventually we choose a cutoff of cook's d <.01. This removes, 162 observations, a mere 0.78% of our data. Yet we have lowered the model variance by an astounding 23.9%!

The residual plot seems to have similar features, though cleaner and zoomed in, showing that we removed extreme values. While we have achieved good results here, this approach feels wrong, ideally we would like to have the model fit even the extreme points, so we attempt a quick and dirty logarithmic model. if we have time we will look at square root as well.

### 7) Log model 
```{r Log model}
h_log <- mutate(h, log_Price=log10(Price), #20800 obs
               log_Rooms=log10(Rooms),
               log_Distance=log10(Distance),
               log_Bedroom2=log10(Bedroom2),
               log_Bathroom=log10(Bathroom) )
h_log <- filter(h_log, h_log$log_Price > -99 & #20748 obs, removes 52.
                h_log$log_Rooms > -99 &
                h_log$log_Distance > -99 &
                h_log$log_Bedroom2 > -99 &
                h_log$log_Bathroom > -99)
m_log <- lm(log_Price ~ log_Rooms+log_Distance+log_Bedroom2+log_Bathroom+Type, data = h_log)
summary(m_log)
glance(m_log)
```
After taking the log of all variables (except Type) we remove values that were 0 (ie, remove (0)=-Inf). The fit the model of all logged variables. R^2= .51! something more surprising is that AIC is now -18,000 and deviance is 500. After a quick sanity check it makes sense that we cannot compare these values to those before the log transformations. This model holds promise from the higher R^2 though, let's check the ggplots, and residuals.

```{r Log ggplots and residuals.}
GGally::ggpairs(h_log[, 6:11]) #takes a bit
hm_log <- augment(m_log, h_log) #housing model
ggplot(hm_log, aes(x=.fitted, y=.resid)) + geom_point(alpha = .05)
```

Histograms of the log variables look much better with the possible exception of Bathrooms. Residuals look great, centered on 0, no non-linear trends, homoscedasticity! We feel great about this model. Feels like we have our model, but we'll quickly take a look at square root as well.


### 8) Square root model 
```{r Square root model}
h_sqrt <- mutate(h, sqrt_Price=(Price)^.5, #20800 obs
               sqrt_Rooms=(Rooms)^.5,
               sqrt_Distance=(Distance)^.5,
               sqrt_Bedroom2=(Bedroom2)^.5,
               sqrt_Bathroom=(Bathroom)^.5 )

m_sqrt <- lm(sqrt_Price ~ sqrt_Rooms+sqrt_Distance+sqrt_Bedroom2+sqrt_Bathroom+Type, data = h_sqrt)
summary(m_sqrt)
glance(m_sqrt)
hm_sqrt <- augment(m_sqrt, h_sqrt) #housing model
ggplot(hm_sqrt, aes(x=.fitted, y=.resid)) + geom_point(alpha = .05)
```

R^2=.48, better than our linear models, worse than our log model, we also note the changing variance in the residual plot. Our model statistics indicate that the full log model is the best. 

### 9) Interpretation of the log model
```{r Interpretation of the log model}
tidy(m_log)
```
We select the full log model as our best model. It gives the highest adj. R^2 and the residual plots are more desirable than the other models for reasons we have discussed above (namely centered at 0, no slope, no non-linear trends, and homoscedasticity). We will list the model in terms of the three different housing Types: house, unit, and town-home.

- Houses
log(Price) = 6.07 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom) 
- Units
log(Price) = 5.88 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom) 
-Town-homes
log(Price) = 5.98 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom)

It's a little difficult to think in log units, keep in mind that each increase in a log unit, is an order of magnitude increase (x10) in the base unit. 
