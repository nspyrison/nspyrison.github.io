---
title: "EDA and Regression Example"
author: "Team: SnickR"
date: "April 2018"
output: #html_document
  html_document:
    toc: true
    number_sections: true
---
```{r options, include=FALSE}
#rmd output document options: html_document, word_document, pdf_document
#see chunk options at: https://yihui.name/knitr/options/
knitr::opts_chunk$set(
  echo=TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 8,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE)
```

# ETC1010 assignment 3, price regression of Melbourne housing data

## Exercise

- Read in the Melbourne housing data (originally pulled from https://www.kaggle.com/anthonypino/melbourne-housing-market)
- Focus ONLY on the variables Price, Rooms, Type, Distance, Bedroom2, Bathroom 
- Make a missing values summary, and a strategy for dealing with the missings
- Make some good plots summarising the relationship between price
- Build your model, explaining your choices of transformations, variable selection, interactions 
- Summarise your final model, and justify why you think it is the best

## The data

This data set contains 34,857 observations of housing information in Melbourne proper from January of 2016. We were asked to look specifically at 6 variables; Price, Rooms, Type, Distance, Bedroom2, Bathroom; let's start with a short description of the variables:

- Price: price in dollars (assumed to be in AUD valued circa Jan 2016)
- Rooms: Number of rooms
- Type: h - house, t - townhouse, u - unit
- Distance: distance from CBD (assumed to be km)
- Bedroom2: Scraped # of Bedrooms (from different source)
- Bathroom: Number of Bathrooms


### Load and subset data 
```{r, results='hide'}
library(readr)
library(tibble)
#h <- readr::read_csv("data/Melbourne_housing_FULL.csv")
#h <- tibble::as_tibble(select(.data = h,
#  c(Price, Rooms, Distance, Bedroom2, Bathroom, Type)
#))
#save(h, file = "./data/Melbourne_housing_subset.rda")
load(file = "./data/Melbourne_housing_subset.rda") 
  #h, a tibble subset of melbourne housing data (Kaggle) ~35k x 6.
h
```

## EDA: missingness and pair-wise distributions.
```{r}
#library(visdat)
# missingness
visdat::vis_dat(h, palette = "cb_safe")
visdat::vis_miss(h, sort_miss = TRUE) + theme(aspect.ratio = 1)
h <- h[complete.cases(h), ] #20800 x 6. lost 14,057 rows, 40.3%

library(GGally)
GGally::ggpairs(h[, 1:6])

```
A fair amount missing. Will use `complete.cases` for now to get moving, ideally we would do imputation (perhaps knn) to estimate the missing values. Using `complete.cases` removes 14,057 observations, some 40.3% of our data. Note that we would still have to throw out 21.8% of our data even with imputation as explanatory variables (y's) shouldn't be imputed. 

We still have a rather large sample size of `n = 20,800` to model off of. We use `GGally::ggpairs()` to explore further, which summarizes numeric variables and categorical variables with a low number of unique levels well. 

Several observations about the pairs:
- Data density prohibits seeing an clearly non-linear trends
-- Later, in vizualizing the data, we will use ggplot2's `alpha` and/or `geom_jitter` to see through some of this density.
- Extreme values especially in Price, Rooms, Bedroom2, and Bathroom
-- We will want to log these variables at some point. We end up comaring with regressions of the linear variables and also with a square-root transform (another way to sqash large numbers). Spoiler: we find that log-scale works better.
- We are looking at mostly house listings (`Type = h`).
- Correlation between Bedroom2 and Rooms is very high, `Corr = .959`. This makes sense, as the number of bedrooms should always be reflected by the number of rooms.
-- In light of the high correlation between these variables is so high, it's likely that one of these variables will not be signigicant in our regression models. ie) the unique information held within number of bedrooms will be small, given that most of the information is already held within the number of rooms.

## Linear models
```{r}
library(broom)
m1 <- lm(Price ~ ., data = h)
summary(m1) # note that Bedroom2 is the only non-significant term.
broom::glance(m1)

m2 <- lm(Price ~ .-Bedroom2, data = h)
summary(m2)
broom::glance(m2)
```
Looking at the `summary()` of the full model, Bedroom2 is the only non-significant variable. This seems quite shocking at first, but viewed in the context that it has corr = .959 with Rooms this seem more intuitive. The other variables are hands down relevant to the model as seen by their near 0 p values; all of which are below 2*10^(-16)!

Viewing `glance()` is less impressive. AIC > 600,000, deviance > 5.15e+15. While discouraging we need to take these in context the range of the Prices is huge, we will have large deviance terms. Adj r^2 = .418, not bad when considering the interpretation: a linear model of 6 (easily observable) variables can describe over 40% of the variation in Price. 

Moving right along, we'll go to our second model, another linear model this time, we remove Bedroom2 from the explainatory varianbles, a negligible loss in informatio to drop 1 of our 6 variables.

```{r}
library(ggplot2)
hm2 <- broom::augment(m2, h) 
  # housing model 2. h, augmented with fitted values and residuals.
ggplot(hm2, aes(x=.fitted, y=.resid)) +
  geom_point(alpha = .1, position = "jitter")
```
Several comments on the residual plot of the 2nd model.

- The fitted contains values well below zero. This intuitively illogical, instead of investing time to implemend a lower bound (or floor price) near 0 or  min(Price) we will instead spend time making other models that won't contain this issue.
- Heteroskedasticity, in the residual plot (changing variance in the residual plot), this is the final nail in the coffin for the linear regression, we have sufficenit edvidence that a non-linear regression will fit the data better.
- The mass of the observations seems to have a negative slope. Extreme values may well be having a huge effect on the sum of squares. 

### Log model 
```{r}
library(dplyr)
h_log <- mutate(.data = h, 
                log_Price = log10(Price), #20800 obs
                log_Rooms = log10(Rooms),
                log_Distance = log10(Distance),
                log_Bedroom2 = log10(Bedroom2),
                log_Bathroom = log10(Bathroom) )
h_log <- filter(.data = h_log, 
                h_log$log_Price > -99 & #20748 obs, removes 52.
                  h_log$log_Rooms > -99 &
                  h_log$log_Distance > -99 &
                  h_log$log_Bedroom2 > -99 &
                  h_log$log_Bathroom > -99)
m_log <- lm(log_Price ~ log_Rooms + log_Distance + log_Bedroom2 + 
              log_Bathroom + Type, data = h_log)
summary(m_log)
broom::glance(m_log)
```
After taking the log of numeric variables we remove values that were 0 (ie, remove (0) = -Inf). Alternatively we could have added 1, or .1 to the variables before log transform. The fit the model of all logged variables. R^2 =  .51! something more surprising is that AIC is now -18,000 and deviance is 500. After a quick sanity check it makes sense AIC anddeviance is so much lower at log(price) is much smaller than price on the scale. This model holds promise from the higher R^2 though, let's check the ggpairs, and residuals.

```{r Log ggplots and residuals.}
GGally::ggpairs(h_log[, 6:11]) #takes a bit
hm_log <- broom::augment(m_log, h_log) #housing model
ggplot(hm_log, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = .1, position = "jitter")
```

Histograms of the log variables look much better with the possible exception of Bathrooms. Residuals look great, centered on 0, no non-linear trends, homoscedasticity! We feel great about this model. Feels like we have our model, but we'll quickly take a look at another way to keep large values in check, via a square root transformation.

### Square root model
```{r}
h_sqrt <- mutate(.data = h, 
                 sqrt_Price = (Price)^.5, #20800 obs
                 sqrt_Rooms = (Rooms)^.5,
                 sqrt_Distance = (Distance)^.5,
                 sqrt_Bedroom2 = (Bedroom2)^.5,
                 sqrt_Bathroom = (Bathroom)^.5 )

m_sqrt <- lm(sqrt_Price ~ sqrt_Rooms + sqrt_Distance + sqrt_Bedroom2 +
               sqrt_Bathroom + Type, data = h_sqrt)
summary(m_sqrt)
broom::glance(m_sqrt)
hm_sqrt <- broom::augment(m_sqrt, h_sqrt)
ggplot(hm_sqrt, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = .1, position = "jitter")
```

R^2 = .48, better than our linear models, worse than our log model, we also note the changing variance in the residual plot. Our model statistics indicate that the full log model is the best. 

## Interpretation of the log model
```{r}
summary(m_log)
```
We select the full log model as our best model. It gives the highest adj. R^2 and the residual plots are more desirable than the other models for reasons we have discussed above (namely centered at 0, no slope, no non-linear trends, and homoscedasticity). We will list the model in terms of the three different housing Types: house, unit, and town-home.

- Houses
log(Price) = 6.07 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom) 
- Units
log(Price) = 5.88 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom) 
- Town-homes
log(Price) = 5.98 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom)

It's a little difficult to think in log units, keep in mind that each increase in a log unit, is an order of magnitude increase (x10) in the base unit.