---
title: "EDA and Regression Example"
author: "Team: SnickR"
date: "April 2018"
output: #html_document
  html_document:
    toc: true
    number_sections: true
---



<div id="etc1010-assignment-3-price-regression-of-melbourne-housing-data" class="section level1">
<h1>ETC1010 assignment 3, price regression of Melbourne housing data</h1>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<ul>
<li>Read in the Melbourne housing data (originally pulled from <a href="https://www.kaggle.com/anthonypino/melbourne-housing-market" class="uri">https://www.kaggle.com/anthonypino/melbourne-housing-market</a>)</li>
<li>Focus ONLY on the variables Price, Rooms, Type, Distance, Bedroom2, Bathroom</li>
<li>Make a missing values summary, and a strategy for dealing with the missings</li>
<li>Make some good plots summarising the relationship between price</li>
<li>Build your model, explaining your choices of transformations, variable selection, interactions</li>
<li>Summarise your final model, and justify why you think it is the best</li>
</ul>
</div>
<div id="the-data" class="section level2">
<h2>The data</h2>
<p>This data set contains 34,857 observations of housing information in Melbourne proper from January of 2016. We were asked to look specifically at 6 variables; Price, Rooms, Type, Distance, Bedroom2, Bathroom; let’s start with a short description of the variables:</p>
<ul>
<li>Price: price in dollars (assumed to be in AUD valued circa Jan 2016)</li>
<li>Rooms: Number of rooms</li>
<li>Type: h - house, t - townhouse, u - unit</li>
<li>Distance: distance from CBD (assumed to be km)</li>
<li>Bedroom2: Scraped # of Bedrooms (from different source)</li>
<li>Bathroom: Number of Bathrooms</li>
</ul>
<div id="load-and-subset-data" class="section level3">
<h3>Load and subset data</h3>
<pre class="r"><code>library(readr)
library(tibble)
#h &lt;- readr::read_csv(&quot;data/Melbourne_housing_FULL.csv&quot;)
#h &lt;- tibble::as_tibble(select(.data = h,
#  c(Price, Rooms, Distance, Bedroom2, Bathroom, Type)
#))
#save(h, file = &quot;./data/Melbourne_housing_subset.rda&quot;)
load(file = &quot;./data/Melbourne_housing_subset.rda&quot;) 
  #h, a tibble subset of melbourne housing data (Kaggle) ~35k x 6.
h</code></pre>
</div>
</div>
<div id="eda-missingness-and-pair-wise-distributions." class="section level2">
<h2>EDA: missingness and pair-wise distributions.</h2>
<pre class="r"><code>#library(visdat)
# missingness
visdat::vis_dat(h, palette = &quot;cb_safe&quot;)</code></pre>
<p><img src="eda+regression_files/figure-html/unnamed-chunk-2-1.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>visdat::vis_miss(h, sort_miss = TRUE) + theme(aspect.ratio = 1)</code></pre>
<p><img src="eda+regression_files/figure-html/unnamed-chunk-2-2.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h &lt;- h[complete.cases(h), ] #20800 x 6. lost 14,057 rows, 40.3%

library(GGally)
GGally::ggpairs(h[, 1:6])</code></pre>
<p><img src="eda+regression_files/figure-html/unnamed-chunk-2-3.png" width="1152" style="display: block; margin: auto;" /> A fair amount missing. Will use <code>complete.cases</code> for now to get moving, ideally we would do imputation (perhaps knn) to estimate the missing values. Using <code>complete.cases</code> removes 14,057 observations, some 40.3% of our data. Note that we would still have to throw out 21.8% of our data even with imputation as explanatory variables (y’s) shouldn’t be imputed.</p>
<p>We still have a rather large sample size of <code>n = 20,800</code> to model off of. We use <code>GGally::ggpairs()</code> to explore further, which summarizes numeric variables and categorical variables with a low number of unique levels well.</p>
<p>Several observations about the pairs: - Data density prohibits seeing an clearly non-linear trends – Later, in vizualizing the data, we will use ggplot2’s <code>alpha</code> and/or <code>geom_jitter</code> to see through some of this density. - Extreme values especially in Price, Rooms, Bedroom2, and Bathroom – We will want to log these variables at some point. We end up comaring with regressions of the linear variables and also with a square-root transform (another way to sqash large numbers). Spoiler: we find that log-scale works better. - We are looking at mostly house listings (<code>Type = h</code>). - Correlation between Bedroom2 and Rooms is very high, <code>Corr = .959</code>. This makes sense, as the number of bedrooms should always be reflected by the number of rooms. – In light of the high correlation between these variables is so high, it’s likely that one of these variables will not be signigicant in our regression models. ie) the unique information held within number of bedrooms will be small, given that most of the information is already held within the number of rooms.</p>
</div>
<div id="linear-models" class="section level2">
<h2>Linear models</h2>
<pre class="r"><code>library(broom)
m1 &lt;- lm(Price ~ ., data = h)
summary(m1) # note that Bedroom2 is the only non-significant term.

Call:
lm(formula = Price ~ ., data = h)

Residuals:
     Min       1Q   Median       3Q      Max 
-2426737  -293255   -75045   197565  9446981 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   652067      15352    42.5   &lt;2e-16 ***
Rooms         168344      13128    12.8   &lt;2e-16 ***
Distance      -38670        526   -73.5   &lt;2e-16 ***
Bedroom2       12820      12806     1.0     0.32    
Bathroom      260776       6378    40.9   &lt;2e-16 ***
Typet        -304850      13552   -22.5   &lt;2e-16 ***
Typeu        -407348      11106   -36.7   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 498000 on 20793 degrees of freedom
Multiple R-squared:  0.418, Adjusted R-squared:  0.418 
F-statistic: 2.49e+03 on 6 and 20793 DF,  p-value: &lt;2e-16
broom::glance(m1)
# A tibble: 1 x 11
  r.squared adj.r.squared  sigma statistic p.value    df  logLik    AIC
*     &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;
1     0.418         0.418 4.98e5     2493.       0     7 -3.02e5 6.05e5
# ... with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;

m2 &lt;- lm(Price ~ .-Bedroom2, data = h)
summary(m2)

Call:
lm(formula = Price ~ . - Bedroom2, data = h)

Residuals:
     Min       1Q   Median       3Q      Max 
-2366092  -293178   -75013   197786  9446925 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   653168      15312    42.7   &lt;2e-16 ***
Rooms         180314       5417    33.3   &lt;2e-16 ***
Distance      -38642        525   -73.6   &lt;2e-16 ***
Bathroom      261465       6341    41.2   &lt;2e-16 ***
Typet        -305286      13545   -22.5   &lt;2e-16 ***
Typeu        -407755      11099   -36.7   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 498000 on 20794 degrees of freedom
Multiple R-squared:  0.418, Adjusted R-squared:  0.418 
F-statistic: 2.99e+03 on 5 and 20794 DF,  p-value: &lt;2e-16
broom::glance(m2)
# A tibble: 1 x 11
  r.squared adj.r.squared  sigma statistic p.value    df  logLik    AIC
*     &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;
1     0.418         0.418 4.98e5     2992.       0     6 -3.02e5 6.05e5
# ... with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>Looking at the <code>summary()</code> of the full model, Bedroom2 is the only non-significant variable. This seems quite shocking at first, but viewed in the context that it has corr = .959 with Rooms this seem more intuitive. The other variables are hands down relevant to the model as seen by their near 0 p values; all of which are below 2*10^(-16)!</p>
<p>Viewing <code>glance()</code> is less impressive. AIC &gt; 600,000, deviance &gt; 5.15e+15. While discouraging we need to take these in context the range of the Prices is huge, we will have large deviance terms. Adj r^2 = .418, not bad when considering the interpretation: a linear model of 6 (easily observable) variables can describe over 40% of the variation in Price.</p>
<p>Moving right along, we’ll go to our second model, another linear model this time, we remove Bedroom2 from the explainatory varianbles, a negligible loss in informatio to drop 1 of our 6 variables.</p>
<pre class="r"><code>library(ggplot2)
hm2 &lt;- broom::augment(m2, h) 
  # housing model 2. h, augmented with fitted values and residuals.
ggplot(hm2, aes(x=.fitted, y=.resid)) +
  geom_point(alpha = .1, position = &quot;jitter&quot;)</code></pre>
<p><img src="eda+regression_files/figure-html/unnamed-chunk-4-1.png" width="1152" style="display: block; margin: auto;" /> Several comments on the residual plot of the 2nd model.</p>
<ul>
<li>The fitted contains values well below zero. This intuitively illogical, instead of investing time to implemend a lower bound (or floor price) near 0 or min(Price) we will instead spend time making other models that won’t contain this issue.</li>
<li>Heteroskedasticity, in the residual plot (changing variance in the residual plot), this is the final nail in the coffin for the linear regression, we have sufficenit edvidence that a non-linear regression will fit the data better.</li>
<li>The mass of the observations seems to have a negative slope. Extreme values may well be having a huge effect on the sum of squares.</li>
</ul>
<div id="log-model" class="section level3">
<h3>Log model</h3>
<pre class="r"><code>library(dplyr)
h_log &lt;- mutate(.data = h, 
                log_Price = log10(Price), #20800 obs
                log_Rooms = log10(Rooms),
                log_Distance = log10(Distance),
                log_Bedroom2 = log10(Bedroom2),
                log_Bathroom = log10(Bathroom) )
h_log &lt;- filter(.data = h_log, 
                h_log$log_Price &gt; -99 &amp; #20748 obs, removes 52.
                  h_log$log_Rooms &gt; -99 &amp;
                  h_log$log_Distance &gt; -99 &amp;
                  h_log$log_Bedroom2 &gt; -99 &amp;
                  h_log$log_Bathroom &gt; -99)
m_log &lt;- lm(log_Price ~ log_Rooms + log_Distance + log_Bedroom2 + 
              log_Bathroom + Type, data = h_log)
summary(m_log)

Call:
lm(formula = log_Price ~ log_Rooms + log_Distance + log_Bedroom2 + 
    log_Bathroom + Type, data = h_log)

Residuals:
   Min     1Q Median     3Q    Max 
-0.948 -0.111 -0.009  0.102  1.082 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   6.06775    0.00591 1026.42  &lt; 2e-16 ***
log_Rooms     0.46681    0.03256   14.34  &lt; 2e-16 ***
log_Distance -0.37113    0.00428  -86.75  &lt; 2e-16 ***
log_Bedroom2  0.11424    0.03216    3.55  0.00038 ***
log_Bathroom  0.24694    0.00767   32.19  &lt; 2e-16 ***
Typet        -0.08739    0.00427  -20.49  &lt; 2e-16 ***
Typeu        -0.18560    0.00368  -50.45  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.157 on 20741 degrees of freedom
Multiple R-squared:  0.51,  Adjusted R-squared:  0.51 
F-statistic: 3.6e+03 on 6 and 20741 DF,  p-value: &lt;2e-16
broom::glance(m_log)
# A tibble: 1 x 11
  r.squared adj.r.squared sigma statistic p.value    df logLik     AIC
*     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1     0.510         0.510 0.157     3603.       0     7  8976. -17936.
# ... with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>After taking the log of numeric variables we remove values that were 0 (ie, remove (0) = -Inf). Alternatively we could have added 1, or .1 to the variables before log transform. The fit the model of all logged variables. R^2 = .51! something more surprising is that AIC is now -18,000 and deviance is 500. After a quick sanity check it makes sense AIC anddeviance is so much lower at log(price) is much smaller than price on the scale. This model holds promise from the higher R^2 though, let’s check the ggpairs, and residuals.</p>
<pre class="r"><code>GGally::ggpairs(h_log[, 6:11]) #takes a bit</code></pre>
<p><img src="eda+regression_files/figure-html/Log%20ggplots%20and%20residuals.-1.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>hm_log &lt;- broom::augment(m_log, h_log) #housing model
ggplot(hm_log, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = .1, position = &quot;jitter&quot;)</code></pre>
<p><img src="eda+regression_files/figure-html/Log%20ggplots%20and%20residuals.-2.png" width="1152" style="display: block; margin: auto;" /></p>
<p>Histograms of the log variables look much better with the possible exception of Bathrooms. Residuals look great, centered on 0, no non-linear trends, homoscedasticity! We feel great about this model. Feels like we have our model, but we’ll quickly take a look at another way to keep large values in check, via a square root transformation.</p>
</div>
<div id="square-root-model" class="section level3">
<h3>Square root model</h3>
<pre class="r"><code>h_sqrt &lt;- mutate(.data = h, 
                 sqrt_Price = (Price)^.5, #20800 obs
                 sqrt_Rooms = (Rooms)^.5,
                 sqrt_Distance = (Distance)^.5,
                 sqrt_Bedroom2 = (Bedroom2)^.5,
                 sqrt_Bathroom = (Bathroom)^.5 )

m_sqrt &lt;- lm(sqrt_Price ~ sqrt_Rooms + sqrt_Distance + sqrt_Bedroom2 +
               sqrt_Bathroom + Type, data = h_sqrt)
summary(m_sqrt)

Call:
lm(formula = sqrt_Price ~ sqrt_Rooms + sqrt_Distance + sqrt_Bedroom2 + 
    sqrt_Bathroom + Type, data = h_sqrt)

Residuals:
   Min     1Q Median     3Q    Max 
-755.7 -133.4  -24.2  106.3 2147.2 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     594.20      11.27   52.74   &lt;2e-16 ***
sqrt_Rooms      304.85      17.38   17.54   &lt;2e-16 ***
sqrt_Distance  -129.56       1.49  -86.71   &lt;2e-16 ***
sqrt_Bedroom2    29.23      16.78    1.74    0.082 .  
sqrt_Bathroom   241.73       6.53   37.00   &lt;2e-16 ***
Typet          -120.81       5.35  -22.57   &lt;2e-16 ***
Typeu          -205.15       4.51  -45.48   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 197 on 20793 degrees of freedom
Multiple R-squared:  0.486, Adjusted R-squared:  0.486 
F-statistic: 3.27e+03 on 6 and 20793 DF,  p-value: &lt;2e-16
broom::glance(m_sqrt)
# A tibble: 1 x 11
  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC
*     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;
1     0.486         0.486  197.     3273.       0     7 -1.39e5 2.79e5
# ... with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;
hm_sqrt &lt;- broom::augment(m_sqrt, h_sqrt)
ggplot(hm_sqrt, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = .1, position = &quot;jitter&quot;)</code></pre>
<p><img src="eda+regression_files/figure-html/unnamed-chunk-6-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>R^2 = .48, better than our linear models, worse than our log model, we also note the changing variance in the residual plot. Our model statistics indicate that the full log model is the best.</p>
</div>
</div>
<div id="interpretation-of-the-log-model" class="section level2">
<h2>Interpretation of the log model</h2>
<pre class="r"><code>summary(m_log)

Call:
lm(formula = log_Price ~ log_Rooms + log_Distance + log_Bedroom2 + 
    log_Bathroom + Type, data = h_log)

Residuals:
   Min     1Q Median     3Q    Max 
-0.948 -0.111 -0.009  0.102  1.082 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   6.06775    0.00591 1026.42  &lt; 2e-16 ***
log_Rooms     0.46681    0.03256   14.34  &lt; 2e-16 ***
log_Distance -0.37113    0.00428  -86.75  &lt; 2e-16 ***
log_Bedroom2  0.11424    0.03216    3.55  0.00038 ***
log_Bathroom  0.24694    0.00767   32.19  &lt; 2e-16 ***
Typet        -0.08739    0.00427  -20.49  &lt; 2e-16 ***
Typeu        -0.18560    0.00368  -50.45  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.157 on 20741 degrees of freedom
Multiple R-squared:  0.51,  Adjusted R-squared:  0.51 
F-statistic: 3.6e+03 on 6 and 20741 DF,  p-value: &lt;2e-16</code></pre>
<p>We select the full log model as our best model. It gives the highest adj. R^2 and the residual plots are more desirable than the other models for reasons we have discussed above (namely centered at 0, no slope, no non-linear trends, and homoscedasticity). We will list the model in terms of the three different housing Types: house, unit, and town-home.</p>
<ul>
<li>Houses log(Price) = 6.07 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom)</li>
<li>Units log(Price) = 5.88 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom)</li>
<li>Town-homes log(Price) = 5.98 + .47 x log(Rooms) -.37 x log(Distance) + .11 x log(Bedroom2) + .25 x log(Bathroom)</li>
</ul>
<p>It’s a little difficult to think in log units, keep in mind that each increase in a log unit, is an order of magnitude increase (x10) in the base unit.</p>
</div>
</div>
